{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizzare statistiche con la TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisiti per il tutorial:**\n",
    "* [T1 - Reti neurali feedforward](1-Reti-neurali-feedforward.ipynb)\n",
    "\n",
    "**Contenuti del tutorial:**\n",
    "1. Struttura della TensorBoard.\n",
    "2. Esempio di utilizzo della TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Come funziona la TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo visto nei tutorial precedenti, gestire quanto succede nelle fasi di allenamento delle reti neurali è un'operazione complessa, resa ulteriormente difficile dalla natura simbolica dei grafi computazionali.\n",
    "\n",
    "La [TensorBoard](https://github.com/tensorflow/tensorboard) è un'applicazione web, a sua volta rilasciata in open source da Google, che mira a rendere più semplice la raccolta e la visualizzazione dei dati ai fini dell'analisi e del debugging. La TensorBoard opera aggiungendo operazioni speciali al grafo computazionale, dette **summary**, che permettono di raccogliere dati di vario tipo (scalari, istogrammi, e così via) e salvarli su file di log tramite un **writer**. Attraverso una dashboard web, infine, è possibile visualizzare interattivamente i dati contenuti nei log.\n",
    "\n",
    "Vediamo di seguito un esempio di utilizzo della TensorBoard allenando una rete neurale su un problema di classificazione, collezionando al contempo l'evoluzione della funzione costo e diverse altre informazioni utili ad analizzarne il comportamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impostiamo il problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo un dataset di benchmark di classificazione tra quelli presenti di default su scikit-learn. Per questo tutorial, useremo l'intero dataset per l'ottimizzazione, senza preoccuparci di overfitting o di valutare l'accuratezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, model_selection\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data['data']\n",
    "y = data['target'].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizializziamo una rete neurale con uno strato nascosto, allo stesso modo di quanto fatto nei tutorial precedenti, cominciando dai placeholder per input ed output desiderato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_tf = tf.placeholder(tf.float32, [None, X.shape[1]])\n",
    "y_tf = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo il primo strato della rete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with tf.name_scope('hidden_layer'):\n",
    "    W1 = tf.Variable(np.random.randn(X.shape[1], 15)*0.01, dtype=tf.float32)\n",
    "    b1 = tf.Variable(np.ones([15])*0.01, dtype=tf.float32)\n",
    "    h = tf.nn.tanh(tf.matmul(X_tf, W1) + b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricordiamo che definire le operazioni all'interno di dei **name scope** permette di visualizzarli in maniera più semplice sul grafo. Continuiamo con lo strato di output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('output_layer'):\n",
    "    w2 = tf.Variable(np.random.randn(15, 1)*0.01, dtype=tf.float32)\n",
    "    b2 = tf.Variable([0.01], dtype=tf.float32)\n",
    "    f_pre = tf.matmul(h, w2) + b2\n",
    "    f = tf.nn.sigmoid(f_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, per finire, la funzione costo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_tf, logits=f_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary e FileWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ciascuna quantità che vogliamo monitorare, dobbiamo definire un'appropriata operazione sul grafo, ovvero una summary. Esistono summary di vario tipo, elencate qui:<br />\n",
    "https://www.tensorflow.org/api_guides/python/summary\n",
    "\n",
    "In questa fase, siamo principalmente interessati a summary scalari ed istogrammi. In particolare, siamo interessati a monitorare l'andamento della funzione costo, oltre a due istogrammi relativi ai pesi della rete neurale, tutte quantità importanti per il debug. La prima permette di valutare la convergenza del processo di ottimizzazione, mentre gli istogrammi danno un'idea complessiva dei valori assunti dalla rete neurale, e permettono di identificare situazioni patologiche quali, per esempio, distribuzioni di pesi fortemente concentrate attorno ad un singolo valore.\n",
    "\n",
    "Iniziamo dal definire un'operazione di summary per la funzione costo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_loss = tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiungiamo poi i summary per le due matrici di pesi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_w1 = tf.summary.histogram('W1', W1)\n",
    "summary_w2 = tf.summary.histogram('w2', w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eseguire i summary individualmente uno per uno è un'operazione tediosa. A questo scopo, TensorFlow mette a disposizione una funzione che ritorna una nuova operazione, la quale permette di calcolare tutti i summary del nostro grafo contemporaneamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oltre a questo, come già detto, abbiamo bisogno di un <code>FileWriter</code> per scrivere i dati risultanti su un file di log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "train_writer = tf.summary.FileWriter('logs', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso possiamo procedere, come in precedenza, inizializzando un algoritmo di ottimizzazione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Adam](https://arxiv.org/abs/1412.6980) è in assoluto uno degli algoritmi più popolari per ottimizzare reti neurali; ne parleremo più avanti. Per ora, continuiamo inizializzando tutte le variabili del grafo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ad ogni iterazione, eseguiamo sia l'operazione di training (<code>train_step</code>), che i nostri summary, tramite l'operazione <code>merged</code>. Usiamo quindi il <code>FileWriter</code> per scrivere i risultati di quest'ultima su disco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(300):\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict={X_tf: X, y_tf: y})\n",
    "    train_writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si noti la sintassi: possiamo eseguire più operazioni in una sola chiamata unendole in una lista. La chiamata restituisce quindi due valori, uno per ciascuna operazione. In questo caso, il risultato di <code>merged</code> viene passato al writer (insieme all'indice dell'iterazione), mentre il secondo valore viene ignorato.\n",
    "\n",
    "Prima di passare alla TensorBoard, possiamo anche salvare il nostro grafo per visualizzarlo a sua volta interattivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lanciamo la TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per lanciare la TensorBoard, eseguiamo il seguente comando da terminale (supponendo di stare nella stessa cartella di questo notebook):\n",
    "\n",
    "<code>tensorboard --logdir=logs</code>\n",
    "\n",
    "Se la pagina non viene immediatamente visualizzata, possiamo raggiungerla da questo indirizzo:\n",
    "\n",
    "http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella prima pagina, possiamo visualizzare tutti i summary relativi a scalari:\n",
    "\n",
    "![TensorBoard_scalar](images/tb_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nella cartella sono presenti più file, vengono visualizzati insieme e possono essere selezionati dal menù in basso a sinistra. Dal grafico, notiamo ad esempio che l'algoritmo è ancora lontano dalla convergenza. Sulla sinistra possiamo inoltre aggiustare il livello di *smoothing* applicato alla curva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spostandoci nella sezione GRAPHS, possiamo visualizzare il grafo computazionele\n",
    "\n",
    "![TensorBoard_scalar](images/tb_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cliccando sul '+' in ciascun name scope, possiamo estenderlo ed esplorare le operazioni al suo interno:\n",
    "\n",
    "![TensorBoard_scalar](images/tb_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella sezione DISTRIBUTIONS, possiamo valutare l'istogramma dei pesi:\n",
    "\n",
    "![TensorBoard_scalar](images/tb_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La linea centrale rappresenta la mediana dei pesi (per ogni iterazione), mentre le zone colorate raggruppano in sequenza il 68esimo percentile, il 95esimo, il 99.7esimo, ed il 100% dei valori. Ad esempio, in questo caso possiamo vedere che i valori si sono spostati da una distribuzione molto vicina a zero (come da inizializzazione), fino ad una distribuzione sempre centrata in zero, ma con valori nell'intervallo [-0.4, +0.4], leggermente asimmetrica verso i valori negativi. Per una visualizzazione più completa, possiamo vedere gli istogrammi nella sezione HISTOGRAMS:\n",
    "\n",
    "![TensorBoard_scalar](images/tb_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per questo tutorial è tutto! Introdurremo altre sezioni più avanzate della TensorBoard nei tutorial successivi."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
